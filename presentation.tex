% !TeX document-id = {2870843d-1baa-4f6a-bd0a-a5c796104a32}
% !BIB TS-program = biber
% !TeX encoding = UTF-8
% TU Delft beamer template

\documentclass[aspectratio=43]{beamer}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{calc}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{MnSymbol,wasysym}
\usepackage{array}
\usepackage{qrcode}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{qcircuit}

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\mode<presentation>{\usetheme[verticalbar=false]{tud}}

% BIB SETTINGS
\usepackage[
    backend=biber,
    giveninits=true,
    maxnames=30,
    maxcitenames=20,
    uniquename=init,
    url=false,
    style=authoryear,
]{biblatex}
\addbibresource{bibfile.bib}
\setlength\bibitemsep{0.3cm} % space between entries in the reference list
\renewcommand{\bibfont}{\normalfont\scriptsize}
\setbeamerfont{footnote}{size=\tiny}
\renewcommand{\cite}[1]{\footnote<.->[frame]{\fullcite{#1}}}
\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\absimage}[4][0.5,0.5]{%
	\begin{textblock}{#3}%width
		[#1]% alignment anchor within image (centered by default)
		(#2)% position on the page (origin is top left)
		\includegraphics[width=#3\paperwidth]{#4}%
\end{textblock}}

\newcommand{\mininomen}[2][1]{{\let\thefootnote\relax%
	\footnotetext{\begin{tabular}{*{#1}{@{\!}>{\centering\arraybackslash}p{1em}@{\;}p{\textwidth/#1-2em}}}%
	#2\end{tabular}}}}

% New commands (Giorgio)
\newcommand{\R}{\mathbb{R}}  % real set
% \newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\title[]{Applied Quantum Algorithms - Lecture 8 - Quantum Neural Networks}
\institute[]{Delft University of Technology, The Netherlands}
\author{Giorgio Tosti Balducci}
\date{Apr 26, 2023}


\begin{document}
\section{Introduction}
{
\setbeamertemplate{footline}{\usebeamertemplate*{minimal footline}}
\frame{\titlepage}
}

\begin{frame}{Contents} % some commands, e.g. \verb require [fragile]
\begin{enumerate}
  \item Quantum machine learning (QML) concepts
  \begin{enumerate}
    \item Review of supervised learning
    \item Review of classical neural networks (NNs)
    \item Why QML?
    \item Quantum neural network model
    \item Embedding classical data into quantum states
    \item Practical matters
  \end{enumerate}
  \item Demos on PennyLane and Qiskit
\end{enumerate}
\end{frame}

\section{Lecture}
\begin{frame}{Supervised learning}
  Aim: Given some data and their \textbf{labels}, predict the function\footnote{A different formulation may be to predict the distributions $p(x,y)$ or $p\left(y|x\right)$.} that assigns \textbf{unseen} data to the correct label.
  \pause
  \begin{table}[htbp]
    \centering
    \begin{tabular}{lc}
      Given &$D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\}$,\\
    where &$S\subseteq \mathbb{R}^n$,\\
    predict &$f:S\rightarrow labels$
    \end{tabular}
  \end{table}
  % \begin{align*}
  %   \mathrm{Given} &D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\},\\
  %   \mathrm{where} &S\subseteq \mathbb{R}^n,\\
  %   \mathrm{predict} &f:S\rightarrow labels
  % \end{align*}

  \pause
  \begin{itemize}
    \item \emph{Classification}: discrete labels
    \item \emph{Regression}: continuous labels
  \end{itemize}
\end{frame}

\begin{frame}{Supervised learning}
  \framesubtitle{function family and training}
	Instead of searching among all possible $f:S\rightarrow labels$, we choose a \emph{function family} or \emph{ansatz}, characterized by \emph{free parameters}.

  \begin{gather*}
    \left\{ f_\theta|\, f_\theta: S, \Theta \rightarrow labels \right\}\\
    \Theta \subseteq \mathbb{R}^p
  \end{gather*}

  \pause
  \begin{block}{Training a supervised learning model - practical definition}
    Find $\theta$ s.t. $f_\theta$ best approximizes $f$ w.r.t. a metric of error or \emph{loss}.
  \end{block}

  \pause
  \begin{table}[h]
    \centering
    \begin{tabular}{ccc}
      Mean squared error & $ \sum_{i=1}^N \left( y_i - f_\theta\left( x_i \right) \right)^2$ & Regression\\
      Hinge loss & $ \sum_{i=1}^N \mathrm{max}\left( 0, 1-y_if(x_i) \right)$& Classification
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Supervised learning algorithms}
  \begin{itemize}
    \item Neural networks (this lecture)
    \item Support vector machines (next lecture)
    \item<2-> Naive Bayes
    \item<2-> Linear regression
    \item<2-> Logistic regression
    \item<2-> $k$-nearest neighbours
    \item<2-> Random forest
    \item<2-> ...
  \end{itemize}
  
  \note{Read some algorithms descriptions \href{here}{https://www.ibm.com/topics/supervised-learning}
  }
\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\textwidth]{tikz/cnn.pdf}
  \end{figure}

  \pause
  \begin{itemize}
    \item $f_\theta = f\left( \mathbf{x}, W^{\left( 1 \right)}, \mathbf{b}^{\left( 1 \right)}W^{\left( 2 \right)}, \mathbf{b}^{\left( 2 \right)}, \dots, W^{\left( l \right)}, \mathbf{b}^{\left( l \right)} \right)$
    \item Layered structure: $\mathbf{x}^{\left( l+1 \right)} = \sigma\left( W^{\left( l \right)}\mathbf{x}^{\left( l \right)} + \mathbf{b}^{\left( l \right)} \right)$
  \end{itemize}

  \pause
  Furthermore, NNs are universal function approximators\cite{nielsenneural}.

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Training - backpropagation}
  
    Training neural networks uses gradient-based algorithms
    \[\theta^{(t+1)} = \theta^{(t)} - \eta\nabla_{\theta^{(t)}}L\]

    \pause
    Once we define
    \begin{enumerate}
      \item free parameters
      \item model
      \item loss function
    \end{enumerate}

    we can build the so-called  \emph{computation graph}.

    \pause
    \centering
    \includegraphics[width=.6\textwidth]{pics/computation-graph.jpeg}
    

\end{frame}


\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Training - backpropagation}

  To build the gradient at time (epoch) $t$, we need 2 steps
  \begin{enumerate}
    \item \emph{forward} direction - get the loss value and evaluate local gradients
      \item \emph{backward} direction: multiplications (chain rule)
  \end{enumerate}

  \pause
  It works differently in QNNs, since \emph{we cannot store intermediate gradients}, due to the no-cloning theorem. More later\dots


\end{frame}


\begin{frame}
  \frametitle{Intermezzo}
  \framesubtitle{Computation graph with QNN}

  

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Backpropagation example}


  \note{leave it blank for drawing the computational graph like in handwritten notes}

\end{frame}


\begin{frame}
  \frametitle{Quantum machine learning}
  \framesubtitle{Some informal definitions}
  
  \footnotesize
  \begin{block}{\textbf{Quantum-enhanced machine learning}}
    Quantum computation to speed-up classical machine learning operations.

    \ \\
    \emph{Example}: SVM with quantum linear systems solver algorithm

    \ \\
    \pause
    \emph{Limitations}
    \begin{itemize}
      \item Input/output problem (QRAM?)
      \item De-quantization argument: for a fair comparison, \emph{both classical and quantum} algorithm must have access to a QRAM $\rightarrow$ classical algorithm also runs in $O\left( \log(N) \right)$
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Quantum machine learning}
  \framesubtitle{Some informal definitions}

  \begin{block}{\textbf{Machine learning in quantum feature spaces}}
    Information is \emph{encoded} into quantum states and classical algorithms find the optimal model.
  \end{block}

  \onslide<2>{\emph{Note}: From here on, we will be focusing on ML in quantum feature spaces. For a review of quantum-enhanced ML, see \cite{Biamonte_2017}}
  

\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{The advantage POV}

  As with all quantum algorithms, we ask ourselves: \emph{how is QML better than ML?}

  \ \\
  \pause
  Quantum advantage can be proved by
  \begin{itemize}
    \item Mapping data to certain states
    \item These states are hard to produce classically, but easy to produce (BQP) via a quantum algorithm
    \begin{itemize}
      \item e.g. by solving the discrete log problem \cite{Liu_2021}
    \end{itemize}
    \item We can compute inner products efficiently\footnote{the key idea of kernel theory, more in the next lecture}
  \end{itemize}



\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{The advantage POV}
  However\dots

  \begin{itemize}
    \item Are there any \emph{practically interesting} dataset in which QML can achieve advantage?
    \pause
    \item Is advantage even the right thing to look for?
  \end{itemize}


\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{Is quantum advantage the right goal for quantum machine learning? \cite{Schuld_2022}}
  
  Advantage is just one of the possible points of view and maybe not the most useful one.
  \begin{enumerate}
    \item<2-> What is(are) the fundamental building block(s) of QML models?
    \begin{itemize}
      \item<2-> a.k.a. what is the \emph{quantum perceptron}?
    \end{itemize}
    \item<3-> Can classical learning theory give us guarantees on the learning performance of QML models?
    \begin{itemize}
      \item<3-> Yes, and the link is kernel theory.
    \end{itemize}
    \item<4-> Can we automate the learning process of (certain) QML models?
    \begin{itemize}
      \item<4-> In particular, automatic differentiation.
    \end{itemize}
  \end{enumerate}

\end{frame}


\begin{frame}
  \frametitle{The quantum neural network model}

  3 ingredients
  \begin{itemize}
    \item A data encoding unitary: $U(\mathbf{x})$
    \item A variational unitary (anzatz): $W\left( \mathbf{\theta} \right)$
    \item An observable (measurement operator): $\mathcal{M}$
  \end{itemize}

  \pause
  \[f(\mathbf{x}, \theta) = \ev{\mathcal{M}}{\psi\left( \mathbf{x}, \theta \right)},\]
  \pause
  where
  \[\ket{\psi\left( \mathbf{x}, \theta \right)} = W\left( \theta \right) U(\mathbf{x}) \ket{0}\]
  and
  \[\mathbf{x}\in \R^m,\: \theta\in \R^p\]

\end{frame}


\begin{frame}
  \frametitle{The quantum neural network model}

  % \begin{figure}[h]
  %   \mbox{
  %   \Qcircuit @C=1em @R=.7em {
  %       \lstick{\ket{0}} & & \multigate{3}{U(\mathbf{x})} & \multigate{3}{W(\theta)} & & \multimeasure{3}{\mathcal{M}}\\
  %       \lstick{\ket{0}} & & \ghost{U(\mathbf{x})} & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}\\
  %       & \vdots & \ghost{U(\mathbf{x})} & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}\\
  %       \lstick{\ket{0}} & & \ghost{U(\mathbf{x}) & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}}
  %       }
      
  % \end{figure}

  \note{left blank for drawing the generic circuit}
        
\end{frame}


\begin{frame}
  \frametitle{A simple QNN example}

  
  \note{draw the simple RX(x)-RZ($\theta$) circuit with X measurement}

\end{frame}


\begin{frame}
  \frametitle{QNN model evaluation}

  The model $f = \ev{\mathcal{M}}_{\mathbf{x},\theta}$ is the expectation value of the observable $\mathcal{M}$ of the state $\psi\left( \mathbf{x}, \mathbf{\theta} \right) $
  \begin{itemize}
    \item<2-> In practice, we can compute $\ev{\mathcal{M}_{\mathbf{x},\theta}}$ if we know how to measure in $\mathcal{M}$ basis
    \item<2-> See previous slide where we measured in the $X$ basis.
    \item<3-> Alternatively, we use Hadamard\footnote{\url{https://en.wikipedia.org/wiki/Hadamard_test_(quantum_computation)}} or SWAP\footnote{\url{https://en.wikipedia.org/wiki/Swap_test}} tests
    \item<4-> Finite sampling: $O(1/\varepsilon^2)$ measurement to estimate $\ev{M}$ up to precision $\varepsilon$
    \begin{itemize}
      \item<4-> Due to Chebyshev's inequality
      \item<4-> In practice, a convergence study is necessary
    \end{itemize}
  \end{itemize}
  

\end{frame}


\begin{frame}
  \frametitle{QNN model training}

  Training is hybrid, as for all variational algorithms. At each epoch
  \pause
  \begin{enumerate}
    \item (Quantum step) \emph{Evaluate} the model $f = \ev{\mathcal{M}}_{\mathbf{x},\theta^{(t)}}$ and maybe $\grad_\theta f$.
    \begin{itemize}
      \item More on gradients later on\dots
    \end{itemize}
    \item (Classical step 1) \emph{Forward step} in the computation graph.
    \begin{itemize}
      \item i.e. we evaluate a loss function $L\left( \theta^{(t)} \right)$ and the local gradients.
    \end{itemize}
    \item (Classical step 2) \emph{Backward step} in the computational graph.
    \item (Classical step 3) \emph{Parameter update}.
    \[\mathrm{GD:}\qquad \theta^{(t+1)} = \theta^{(t)} - \eta\grad_{\theta^{(t)}} L\]
  \end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{QNN model training}

  \centering
  \color{red} Do you think it is a good idea to evaluate $\grad_\theta f$ with finite differences? Why? Why not?

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}

  \onslide<2>{A good choice of data embedding can make the difference in terms of}

  \begin{itemize}
    \item<3-> Runtime of the QML routine
    \begin{itemize}
      \item<3-> how does the number of operations scale in the number of features $N$ and dataset dimension $M$
    \end{itemize}
    \item<4-> Learning performance
    \begin{itemize}
      \item<4-> Encoding data means to \emph{map them to a high-dimensional Hilbert space}
      \item<4-> E.g., in classification, we can linearly separate the data of different classes\footnote{even better, with high \emph{margin}}, but also completely shuffle them!
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Data encoding as a feature map}

  \[\phi\left( \mathbf{x} \right) : \mathcal{X}\rightarrow \mathcal{F}\subseteq \mathcal{H}^{2^n}\]

  \ \\
  \centering
  \includegraphics[width=.6\textwidth]{pics/feature-maps.png}
\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Amplitude encoding}

  \pause
  We have $\mathbf{x}\in \R^N$. To make it a valid quantum state:
  \begin{enumerate}
    \item Normalize: $\frac{\mathbf{x}}{\norm{x}_2}$
    \item Pad until the closest power of 2: $x_N,\dots,x_{2^n-1}=0$
  \end{enumerate}

  \pause
  Amplitude encoding (AE) is the identity transformation of this preprocessed state
  \[\phi(\mathbf{x})=\ket{x}=\sum_{i=0}^{N-1}x_i\ket{i}\]

  \pause
  \begin{itemize}
    \item AE is a linear transformation: if data is not linearly separable in the (normalized and padded) original space, then it won't be in the mapped space.
    \item num. qubits: $\log(N)$
    \item runtime: $O(N)$\cite{mottonen2004transformation} \emph{in general}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Amplitude encoding}

  By adding an additional $\log(M)$ qubits, we can also encode data in superposition

  \[\ket{\psi_\mathbf{X}} = \frac{1}{\sqrt{M}}\sum_{m=0}^{M-1}\sum_{i=0}^{N-1} x_i\ket{i}\ket{m} = \frac{1}{\sqrt{M}}\sum_{m=0}^{M-1} \ket{\psi_{\mathbf{x}_m}}\ket{m}\]

  \begin{itemize}
    \item num. qubits: $\log(NM)$
    \item runtime: $O(NM)$
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  \pause
  For basis encoding, we first need to choose a binary representation of the features $x_i$. E.g., in floating point
  \[b_i=b_{i,s} b_{i,n_e-1}\dots b_{i,0}b_{i,n_m-1}\dots b_{i,0}\]

  \ \\
  \pause
  With $\tau = 1+n_e+n_m$ we can encode \emph{each feature of each datapoint} into a different basis state. If we concatenate the features:
  \[\phi(\mathbf{x})=\ket{b_0\dots b_{N-1}}\]

  \ \\
  \pause
  Of course, we can also create superpositions
  \[\ket{\psi_\mathbf{X}} = \frac{1}{\sqrt{M}}\sum_{i=1}^M \ket{\mathbf{x}^{(i)}} \]

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  \begin{minipage}{\textwidth}
    \begin{table}[t]
      \footnotesize
      \centering\begin{tabular}{ccc}
        & Single sample & Dataset\\
        \hline\\
        num. qubits & $N\tau$ & $N\tau$\\
        runtime & $O(N\tau)$ & $O\left( Mn\tau \right)$\cite{ventura1998quantum}\\
        \hline
        
      \end{tabular}
    
    \end{table}
    
  \end{minipage}

  \vspace{3cm}
  \begin{minipage}{\textwidth}
    \note{here include a simple example for basis encoding. For instance $\mathbf{x} = (101, 001)$}
    
  \end{minipage}
  

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  \centering
  \color{red} If basis encoding achieves perfect orthogonality between datapoints, why shouldn't we always use it for classification?

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Time-evolution encoding}

  \pause
  We're not constrained to \emph{exactly} representing features. For instance, each feature can be the \emph{evolution time of a certain Hamiltonian}
  \[\phi(\mathbf{x})=e^{-iH\mathbf{x}}\ket{0}\]

  \ \\
  \pause
  Typically, we do \emph{angle encoding}, i.e. each feature is encoded as a Pauli rotation.

  \ \\
  \pause
  \emph{Example}: one feature, per rotation, per qubit, using Pauli $X$
  \[\phi(\mathbf{x})=e^{-iXx_{N-1}}\otimes e^{-iXx_{N-2}}\otimes\dots\otimes e^{-iXx_0}\ket{0}\]

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Time-evolution encoding}

  For angle encoding:

  \begin{minipage}{\textwidth}
    \begin{table}[t]
      \footnotesize
      \centering\begin{tabular}{ccc}
        & Single sample & Dataset\\
        \hline\\
        num. qubits & $N$ & $N\ceil*{\log(M)}$\\
        runtime & $O(N)$ & $O\left( MN \right)$\\
        \hline
        
      \end{tabular}
    
    \end{table}
    
  \end{minipage}

  \pause
  \vspace{3cm}
  \begin{minipage}{\textwidth}
    \note{include example of rotation encoding with 3 qubits}
  \end{minipage}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Hamiltonian encoding of a dataset}

  \pause
  If we keep the original data matrix $\mathbf{X}\in\R^{M\times N}$ and we can rewrite it as an Hermitian matrix $H_\mathbf{X}$, then we can encode the entire dataset as a Hamiltonian simulation
  \[\phi:\ket{0} \rightarrow e^{-iH_{\mathbf{X}}t}\]

  \begin{itemize}
    \item<3-> The general way to simulate $H_{\mathbf{X}}$ is to use \emph{Trotterization}\cite{lloyd_1996}
    \begin{itemize}
      \item<4-> \emph{Requirement}: we need to know how to decompose $H_{\mathbf{X}}$ in terms that are \emph{easy to simulate}.
      \item<4-> We can always decompose an Hermitian matrix in $O(MN)$ Pauli strings.
    \end{itemize}
    \item<5-> More efficient algorithms exist for \emph{sparse} Hamiltonians, but they require an \emph{oracle}
    \begin{itemize}
      \item<6-> Row $i$, $j$th non-zero element
      \[\ket{i,j,0}\rightarrow \ket{i,j,H_{ij}}\]
      \item<6-> Likely need to hard-code it and full of multi-controlled gates
    \end{itemize}
  \end{itemize}
  
\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Hamiltonian encoding of a dataset}

  \begin{itemize}
    \item Num. qubits: $\log(N)$
    \item Runtime: $O(MN)$ or $O(\log(MN))$
    \begin{itemize}
      \item $O(\log(MN))$ only for sparse $\mathbf{X}$ and a few other cases.
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Quantum models as sums of periodic functions}
  
  \footnotesize
  \pause
  \begin{enumerate}
    \item time-evolution embedding: $S(x_i)= e^{iH x_i}$ for feature $i$.
    \item \emph{layered} model circuit
    \[U(\mathbf{x}, \theta) = W_{N-1}\left( \theta_{N-1} \right)\prod_{i=N-2}^{0}S_i(x_i)W_i\left( \theta_i \right)\]
    \item $H =\mathrm{diag\left( \lambda^i_0,\dots \lambda^i_{d-1} \right)}$
  \end{enumerate}

  \pause
  Then, the quantum model $f(\mathbf{x},\theta) = \ev{\mathcal{M}}{\psi\left( \mathbf{x}, \theta \right)}$ can be expressed as a multidimensional series of periodic functions\cite{Schuld_2021}
  \[f(\mathbf{x},\theta) = \sum_{\omega_0\in \Omega_0}\dots \sum_{\omega_{N-1}\in \Omega_{N-1}} c_{\omega_0\dots \omega_{N-1}}\left( \theta \right)e^{i\omega_0x_0}\dots e^{i\omega_{N-1}x_{N-1}},\]

  \pause
  where \textbf{the frequencies are completely determined by the eigenvalues of the data-encoding Hamiltonian}
  \[\Omega_i=\left\{ \lambda_s^i-\lambda_t^i|s,t\in\left\{ 1,\dots d \right\} \right\}\]

  

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Quantum models as sums of periodic functions}
  
  \footnotesize
  Furthermore, if $\lambda_s^i, \lambda_t^i$ are integer (s.a. for Pauli gates), $f(\mathbf{x},\theta)$ becomes a \emph{Fourier series}
  \[f(\mathbf{x},\theta) = \sum_{n_0\in \Omega_0}\dots \sum_{n_{N-1}\in \Omega_{N-1}} c_{n_0,\dots n_{N-1}}\left( \theta \right) e^{in_0 x_0}\dots e^{in_{N-1} x_{N-1}}\]

  \pause
  In particular, by repeating the encoding of a feature, we can obtain multiple of the period.
  \ \\
  For instance, if two of the features are the same, then $x_i=x_j=x$
  \[e^{in_i x_i}e^{in_j x_j} = e^{i\left( n_i+n_j \right) x}\] 

  \ \\
  \pause
  From this observation and the universality of Fourier series follows the \emph{universality of quantum models with time evolution encoding}.

\end{frame}


\begin{frame}
  \frametitle{Practical matters}
  \framesubtitle{Access to Fourier coefficients in time evolution encoding}
  
  \footnotesize
  What about the coefficients $c_{n_0,\dots n_{N-1}}\left( \theta \right)$?
  \begin{itemize}
    \item<2-> All the elements of the circuit model determine them (data encoding, ansatz, measurement).
    \item<3-> No matter the frequency that we encode for $x_i$, if some $c_{n_i}=0$, then we cannot \emph{access} those frequencies.
    \item<4-> How each element affects each coefficient is a matter of active research.
  \end{itemize}

  \centering
  \onslide<4>{\includegraphics[width=.6\textwidth]{pics/coefficients-access.png}}
  
\end{frame}


\begin{frame}
  \frametitle{Practical matters}
  \framesubtitle{Gradients}
  
  Computing gradients is done with the \emph{parameter shift rule} (PSR). E.g. for Pauli gates:
  \[\pdv{f}{\theta_i}=\frac{f\left( \theta_i + s \right) - f\left( \theta_i - s \right)}{2}\]

  \pause
  How many \emph{model evaluations} we need to compute the \emph{full gradient}?

  \pause
  \vfill
  \begin{table}[htbp]
    \centering\begin{tabular}{cc}
      backpropagation & 2 \\
      PSR & $ 2 n_{\mathrm{params}} n_{\mathrm{shots}}$
    \end{tabular}
  \end{table}

\end{frame}


\begin{frame}
  \frametitle{Practical matters}
  \framesubtitle{Gradients}

  \centering
  \textcolor{red}{Is this a fair comparison?}

  \pause
  \vspace{2em}
  Not really\dots

  \footnotesize
  \begin{table}[htbp]
    \centering
    \begin{tabular}{ccccc}
      \hline\\
      & n. steps& $O(N)$ linear algebra & parallelization & optimized libraries\\
      \hline\\
      Classical & + & - & ++ & ++ \\
      Quantum & -- & + & + & +\\
      \hline
    \end{tabular}
  \end{table}

\end{frame}


\begin{frame}
  \frametitle{Practical matters}
  \framesubtitle{Barren plateaus}
  
  \centering
  \includegraphics[width=.55\textwidth]{pics/bp.jpeg}

  \footnotesize
  \pause
  \begin{align*}
    \mathrm{Var}\left[ \partial_{\theta_i} C\left( \theta \right) \right]\leq F(n), \\
    F(n)\in O\left( \frac{1}{b^n} \right), \: b>1
  \end{align*}

  \pause
  \begin{itemize}
    \item Different reasons, the main one is the ansatz $W(\theta)$
    \item \emph{Problem-agnostic} ansatze suffer from BPs with high $n$ and high depths\cite{McClean_2018}
  \end{itemize}
  
  

\end{frame}

% \section{Conclusion}
% \begin{frame}[fragile]{animation}
%   \vfill
%   Some commands take optional arguments in the form of \verb|<x-y>|,
%   where \verb|x| is the first `sub-frame' on which the context is shown,
%   and \verb|y| is the last. \verb|x| or \verb|y| can be replaced by \verb|+|,
%   referring to `the next sub-frame'. 
%   \vfill
%   \begin{columns}[onlytextwidth]
%   \begin{column}{.5\textwidth}
%     \begin{enumerate}
%       \item<+-> uncovered\ldots
%       \item<+-> one\ldots
%       \item<+-> by\ldots
%       \item<+-> one.
%     \end{enumerate}
%     \end{column}
%   \begin{column}{.5\textwidth}
%       Using only:\only<1>{1}\only<2>{2}\only<3>{3}

%       Using onslide:\onslide<1>{1}\onslide<2>{2}\onslide<3>{3}

%       Using pause:\pause1\pause2\pause3
%   \end{column}
%   \end{columns}
%   \vfill
%   For more advanced animations, see \S 14 of the manual:\\
%   \url{https://www.ctan.org/pkg/beamer}
%   % \url{https://www.ctan.org/pkg/animate}\\
%   % \url{https://www.ctan.org/pkg/media9}
%   \vfill
%   % \transduration{2} automatic progression of slides
%   \transpush<1>
% \end{frame}

\begin{frame}
  \frametitle{Links for demos}

  \vfill
  \centering
  \url{https://colab.research.google.com/github/debrevitatevitae/LabSessionQML/blob/main/variational-classifier-parity.ipynb} 
  \vfill  
  \qrcode{https://colab.research.google.com/github/debrevitatevitae/LabSessionQML/blob/main/variational-classifier-parity.ipynb}
  \vfill
\end{frame}


\begin{frame}[allowframebreaks,t]{\bibname}
	% the 'I' is caused by 'allowframebreaks'
	\AtNextBibliography{\footnotesize}% or in the preamble \AtBeginBibliography{\small}
	\printbibliography
\end{frame}


\end{document}

