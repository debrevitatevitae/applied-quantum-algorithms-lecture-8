% !TeX document-id = {2870843d-1baa-4f6a-bd0a-a5c796104a32}
% !BIB TS-program = biber
% !TeX encoding = UTF-8
% TU Delft beamer template

\documentclass[aspectratio=43]{beamer}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{calc}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{MnSymbol,wasysym}
\usepackage{array}
\usepackage{qrcode}
\usepackage{hyperref}

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\mode<presentation>{\usetheme[verticalbar=false]{tud}}

% BIB SETTINGS
\usepackage[
    backend=biber,
    giveninits=true,
    maxnames=30,
    maxcitenames=20,
    uniquename=init,
    url=false,
    style=authoryear,
]{biblatex}
\addbibresource{bibfile.bib}
\setlength\bibitemsep{0.3cm} % space between entries in the reference list
\renewcommand{\bibfont}{\normalfont\scriptsize}
\setbeamerfont{footnote}{size=\tiny}
\renewcommand{\cite}[1]{\footnote<.->[frame]{\fullcite{#1}}}
\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\absimage}[4][0.5,0.5]{%
	\begin{textblock}{#3}%width
		[#1]% alignment anchor within image (centered by default)
		(#2)% position on the page (origin is top left)
		\includegraphics[width=#3\paperwidth]{#4}%
\end{textblock}}

\newcommand{\mininomen}[2][1]{{\let\thefootnote\relax%
	\footnotetext{\begin{tabular}{*{#1}{@{\!}>{\centering\arraybackslash}p{1em}@{\;}p{\textwidth/#1-2em}}}%
	#2\end{tabular}}}}

\title[]{Applied Quantum Algorithms - Lecture 8 - Quantum Machine Learning}
\institute[]{Delft University of Technology, The Netherlands}
\author{Giorgio Tosti Balducci}
\date{Apr 26, 2023}


\begin{document}
\section{Introduction}
{
\setbeamertemplate{footline}{\usebeamertemplate*{minimal footline}}
\frame{\titlepage}
}

\begin{frame}{Contents} % some commands, e.g. \verb require [fragile]
\begin{enumerate}
  \item Quantum machine learning (QML) concepts
  \begin{enumerate}
    \item Review of supervised learning
    \item Review of classical neural networks (NNs)
    \item Quantum neural network model
    \item Embedding data in quantum states
    \item Why QML?
    \item Physics-informed neural networks (PINNs): classical and quantum 
  \end{enumerate}
  \item Demos on PennyLane and Qiskit
\end{enumerate}
\end{frame}

\section{Lecture}
\begin{frame}{Supervised learning}
  Aim: Given some data and their \textbf{labels}, predict the function\footnote{A more general aim is to predict the posterior probability $p\left(y|x\right)$.} that assing \textbf{unseen} data to the correct label.
  \begin{align}
    \mathrm{Given}& D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\},\\
    \mathrm{where}& S\subseteq \mathbb{R}^n,\\
    \mathrm{predict}& f:S\rightarrow labels\\
  \end{align}

  \begin{itemize}
    \item \emph{Classification}: discrete labels
    \item \emph{Regression}: continuous labels
  \end{itemize}
\end{frame}

\begin{frame}{Supervised learning}
  \framesubtitle{function family and training}
	Instead of searching among all possible $f:S\rightarrow labels$, we choose a \emph{function family} or \emph{ansatz}, characterized by \emph{free parameters}.

  \begin{gather*}
    \left\{ f^\theta|\, f^\theta: S, \Theta \rightarrow labels \right\}\\
    \Theta \subseteq \mathbb{R}^p
  \end{gather*}

  \begin{block}{Training a supervised learning model - practical definition}
    Find $\theta$ s.t. $f_\theta$ best approximizes $f$ w.r.t. a metric of error or \emph{loss}.
  \end{block}

  \begin{table}[h]
    \centering
    \begin{tabular}{ccc}
      Mean squared error & $ \sum_{i=1}^N \left( y_i - f_\theta\left( x_i \right) \right)^2$ & Regression\\
      Negative log-likelihood & $ -\log\left( \mathrm{smtg} \right)$& Classification
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Supervised learning algorithms}
  \begin{itemize}
    \item Neural networks (this lecture)
    \item Support vector machines (next lecture)
    \item Naive bayes
    \item Linear Regression
    \item Logistic regression
    \item $k$-nearest neighbours
    \item Random forest
    \item ...
  \end{itemize}
  
  \note{Read some algorithms descriptions \href{here}{https://www.ibm.com/topics/supervised-learning}
  }
\end{frame}


\section{Conclusion}
\begin{frame}[fragile]{animation}
  \vfill
  Some commands take optional arguments in the form of \verb|<x-y>|,
  where \verb|x| is the first `sub-frame' on which the context is shown,
  and \verb|y| is the last. \verb|x| or \verb|y| can be replaced by \verb|+|,
  referring to `the next sub-frame'. 
  \vfill
  \begin{columns}[onlytextwidth]
  \begin{column}{.5\textwidth}
    \begin{enumerate}
      \item<+-> uncovered\ldots
      \item<+-> one\ldots
      \item<+-> by\ldots
      \item<+-> one.
    \end{enumerate}
    \end{column}
  \begin{column}{.5\textwidth}
      Using only:\only<1>{1}\only<2>{2}\only<3>{3}

      Using onslide:\onslide<1>{1}\onslide<2>{2}\onslide<3>{3}

      Using pause:\pause1\pause2\pause3
  \end{column}
  \end{columns}
  \vfill
  For more advanced animations, see \S 14 of the manual:\\
  \url{https://www.ctan.org/pkg/beamer}
  % \url{https://www.ctan.org/pkg/animate}\\
  % \url{https://www.ctan.org/pkg/media9}
  \vfill
  % \transduration{2} automatic progression of slides
  \transpush<1>
\end{frame}

\begin{frame}
  Thanks for your attention.

  A digital version of this presentation can be found here:
  \vfill
  \url{https://gitlab.com/novanext/tudelft-beamer} 
  \vfill  
  \centering
  \qrcode{https://gitlab.com/novanext/tudelft-beamer}
  \vfill
\end{frame}


\begin{frame}[allowframebreaks,t]{\bibname}
	% the 'I' is caused by 'allowframebreaks'
	\AtNextBibliography{\footnotesize}% or in the preamble \AtBeginBibliography{\small}
	\printbibliography
\end{frame}


\end{document}

