% !TeX document-id = {2870843d-1baa-4f6a-bd0a-a5c796104a32}
% !BIB TS-program = biber
% !TeX encoding = UTF-8
% TU Delft beamer template

\documentclass[aspectratio=43]{beamer}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{calc}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{MnSymbol,wasysym}
\usepackage{array}
\usepackage{qrcode}
\usepackage{hyperref}

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\mode<presentation>{\usetheme[verticalbar=false]{tud}}

% BIB SETTINGS
\usepackage[
    backend=biber,
    giveninits=true,
    maxnames=30,
    maxcitenames=20,
    uniquename=init,
    url=false,
    style=authoryear,
]{biblatex}
\addbibresource{bibfile.bib}
\setlength\bibitemsep{0.3cm} % space between entries in the reference list
\renewcommand{\bibfont}{\normalfont\scriptsize}
\setbeamerfont{footnote}{size=\tiny}
\renewcommand{\cite}[1]{\footnote<.->[frame]{\fullcite{#1}}}
\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\absimage}[4][0.5,0.5]{%
	\begin{textblock}{#3}%width
		[#1]% alignment anchor within image (centered by default)
		(#2)% position on the page (origin is top left)
		\includegraphics[width=#3\paperwidth]{#4}%
\end{textblock}}

\newcommand{\mininomen}[2][1]{{\let\thefootnote\relax%
	\footnotetext{\begin{tabular}{*{#1}{@{\!}>{\centering\arraybackslash}p{1em}@{\;}p{\textwidth/#1-2em}}}%
	#2\end{tabular}}}}

% New commands (Giorgio)
\newcommand{\R}{\mathbb{R}}  % real set
\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}


\title[]{Applied Quantum Algorithms - Lecture 8 - Quantum Machine Learning}
\institute[]{Delft University of Technology, The Netherlands}
\author{Giorgio Tosti Balducci}
\date{Apr 26, 2023}


\begin{document}
\section{Introduction}
{
\setbeamertemplate{footline}{\usebeamertemplate*{minimal footline}}
\frame{\titlepage}
}

\begin{frame}{Contents} % some commands, e.g. \verb require [fragile]
\begin{enumerate}
  \item Quantum machine learning (QML) concepts
  \begin{enumerate}
    \item Review of supervised learning
    \item Review of classical neural networks (NNs)
    \item Why QML?
    \item Quantum neural network model
    \item Embedding classical data into quantum states
    \item Practical matters
    \item Physics-informed neural networks (PINNs): classical and quantum 
  \end{enumerate}
  \item Demos on PennyLane and Qiskit
\end{enumerate}
\end{frame}

\section{Lecture}
\begin{frame}{Supervised learning}
  Aim: Given some data and their \textbf{labels}, predict the function\footnote{A more general aim is to predict the posterior probability $p\left(y|x\right)$.} that assing \textbf{unseen} data to the correct label.
  \begin{align}
    \mathrm{Given}& D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\},\\
    \mathrm{where}& S\subseteq \mathbb{R}^n,\\
    \mathrm{predict}& f:S\rightarrow labels\\
  \end{align}

  \begin{itemize}
    \item \emph{Classification}: discrete labels
    \item \emph{Regression}: continuous labels
  \end{itemize}
\end{frame}

\begin{frame}{Supervised learning}
  \framesubtitle{function family and training}
	Instead of searching among all possible $f:S\rightarrow labels$, we choose a \emph{function family} or \emph{ansatz}, characterized by \emph{free parameters}.

  \begin{gather*}
    \left\{ f^\theta|\, f^\theta: S, \Theta \rightarrow labels \right\}\\
    \Theta \subseteq \mathbb{R}^p
  \end{gather*}

  \begin{block}{Training a supervised learning model - practical definition}
    Find $\theta$ s.t. $f_\theta$ best approximizes $f$ w.r.t. a metric of error or \emph{loss}.
  \end{block}

  \begin{table}[h]
    \centering
    \begin{tabular}{ccc}
      Mean squared error & $ \sum_{i=1}^N \left( y_i - f_\theta\left( x_i \right) \right)^2$ & Regression\\
      Negative log-likelihood & $ -\log\left( \mathrm{smtg} \right)$& Classification
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Supervised learning algorithms}
  \begin{itemize}
    \item Neural networks (this lecture)
    \item Support vector machines (next lecture)
    \item Naive Bayes
    \item Linear regression
    \item Logistic regression
    \item $k$-nearest neighbours
    \item Random forest
    \item ...
  \end{itemize}
  
  \note{Read some algorithms descriptions \href{here}{https://www.ibm.com/topics/supervised-learning}
  }
\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\textwidth]{tikz/cnn.pdf}
  \end{figure}

  \begin{itemize}
    \item $f_\theta = f\left( W^{\left( 1 \right)}, b^{\left( 1 \right)}W^{\left( 2 \right)}, b^{\left( 2 \right)}, \dots, W^{\left( l \right)}, b^{\left( l \right)} \right)$
    \item Layered structure: $x^{\left( l+1 \right)} = \sigma\left( W^{\left( l \right)}x^{\left( l \right)} + b^{\left( l \right)} \right)$
  \end{itemize}

  Furthermore, NNs are universal function approximators\cite{nielsenneural}.

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Training - backpropagation}
  
  backpropagation
  \begin{itemize}
    \item Training neural networks uses gradient-based algorithms
    $$
      \theta^{(t+1)} = \theta^{(t)} - \eta\nabla\theta^{(t)}
    $$
    \item Computation graph: 3 steps
    \begin{itemize}
      \item forward direction - get the loss value
      \item evaluate local gradients
      \item backward direction: multiplications (chain rule)
    \end{itemize}
    \item This is different than QNNs, which breaks the computational graph at the model level
    \item \begin{itemize}
      \item More later \dots
    \end{itemize}
    \note{for a qnn, we need to evaluate so many quantum circuits: $n_{parameters} * 2 * n_{shots}$, where 2 means the evaluation for $\pm \frac{\pi}{2}$}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Backpropagation example}


  \note{leave it blank for drawing the computational graph}

\end{frame}


\section{Conclusion}
\begin{frame}[fragile]{animation}
  \vfill
  Some commands take optional arguments in the form of \verb|<x-y>|,
  where \verb|x| is the first `sub-frame' on which the context is shown,
  and \verb|y| is the last. \verb|x| or \verb|y| can be replaced by \verb|+|,
  referring to `the next sub-frame'. 
  \vfill
  \begin{columns}[onlytextwidth]
  \begin{column}{.5\textwidth}
    \begin{enumerate}
      \item<+-> uncovered\ldots
      \item<+-> one\ldots
      \item<+-> by\ldots
      \item<+-> one.
    \end{enumerate}
    \end{column}
  \begin{column}{.5\textwidth}
      Using only:\only<1>{1}\only<2>{2}\only<3>{3}

      Using onslide:\onslide<1>{1}\onslide<2>{2}\onslide<3>{3}

      Using pause:\pause1\pause2\pause3
  \end{column}
  \end{columns}
  \vfill
  For more advanced animations, see \S 14 of the manual:\\
  \url{https://www.ctan.org/pkg/beamer}
  % \url{https://www.ctan.org/pkg/animate}\\
  % \url{https://www.ctan.org/pkg/media9}
  \vfill
  % \transduration{2} automatic progression of slides
  \transpush<1>
\end{frame}

\begin{frame}
  Thanks for your attention.

  A digital version of this presentation can be found here:
  \vfill
  \url{https://gitlab.com/novanext/tudelft-beamer} 
  \vfill  
  \centering
  \qrcode{https://gitlab.com/novanext/tudelft-beamer}
  \vfill
\end{frame}


\begin{frame}[allowframebreaks,t]{\bibname}
	% the 'I' is caused by 'allowframebreaks'
	\AtNextBibliography{\footnotesize}% or in the preamble \AtBeginBibliography{\small}
	\printbibliography
\end{frame}


\end{document}

