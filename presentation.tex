% !TeX document-id = {2870843d-1baa-4f6a-bd0a-a5c796104a32}
% !BIB TS-program = biber
% !TeX encoding = UTF-8
% TU Delft beamer template

\documentclass[aspectratio=43]{beamer}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{calc}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{MnSymbol,wasysym}
\usepackage{array}
\usepackage{qrcode}
\usepackage{hyperref}
\usepackage{physics}
\usepackage{qcircuit}

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\mode<presentation>{\usetheme[verticalbar=false]{tud}}

% BIB SETTINGS
\usepackage[
    backend=biber,
    giveninits=true,
    maxnames=30,
    maxcitenames=20,
    uniquename=init,
    url=false,
    style=authoryear,
]{biblatex}
\addbibresource{bibfile.bib}
\setlength\bibitemsep{0.3cm} % space between entries in the reference list
\renewcommand{\bibfont}{\normalfont\scriptsize}
\setbeamerfont{footnote}{size=\tiny}
\renewcommand{\cite}[1]{\footnote<.->[frame]{\fullcite{#1}}}
\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\absimage}[4][0.5,0.5]{%
	\begin{textblock}{#3}%width
		[#1]% alignment anchor within image (centered by default)
		(#2)% position on the page (origin is top left)
		\includegraphics[width=#3\paperwidth]{#4}%
\end{textblock}}

\newcommand{\mininomen}[2][1]{{\let\thefootnote\relax%
	\footnotetext{\begin{tabular}{*{#1}{@{\!}>{\centering\arraybackslash}p{1em}@{\;}p{\textwidth/#1-2em}}}%
	#2\end{tabular}}}}

% New commands (Giorgio)
\newcommand{\R}{\mathbb{R}}  % real set
% \newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}


\title[]{Applied Quantum Algorithms - Lecture 8 - Quantum Neural Networks}
\institute[]{Delft University of Technology, The Netherlands}
\author{Giorgio Tosti Balducci}
\date{Apr 26, 2023}


\begin{document}
\section{Introduction}
{
\setbeamertemplate{footline}{\usebeamertemplate*{minimal footline}}
\frame{\titlepage}
}

\begin{frame}{Contents} % some commands, e.g. \verb require [fragile]
\begin{enumerate}
  \item Quantum machine learning (QML) concepts
  \begin{enumerate}
    \item Review of supervised learning
    \item Review of classical neural networks (NNs)
    \item Why QML?
    \item Quantum neural network model
    \item Embedding classical data into quantum states
    \item Practical matters
  \end{enumerate}
  \item Demos on PennyLane and Qiskit
\end{enumerate}
\end{frame}

\section{Lecture}
\begin{frame}{Supervised learning}
  Aim: Given some data and their \textbf{labels}, predict the function\footnote{A different formulation may be to predict the distributions $p(x,y)$ or $p\left(y|x\right)$.} that assing \textbf{unseen} data to the correct label.
  \begin{table}[htbp]
    \centering
    \begin{tabular}{lc}
      Given &$D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\}$,\\
    where &$S\subseteq \mathbb{R}^n$,\\
    predict &$f:S\rightarrow labels$
    \end{tabular}
  \end{table}
  % \begin{align*}
  %   \mathrm{Given} &D=\left\{ \left( x_i,\, y_i \right)|\,x_i\in S;\, y_i=f(x_i) \right\},\\
  %   \mathrm{where} &S\subseteq \mathbb{R}^n,\\
  %   \mathrm{predict} &f:S\rightarrow labels
  % \end{align*}

  \begin{itemize}
    \item \emph{Classification}: discrete labels
    \item \emph{Regression}: continuous labels
  \end{itemize}
\end{frame}

\begin{frame}{Supervised learning}
  \framesubtitle{function family and training}
	Instead of searching among all possible $f:S\rightarrow labels$, we choose a \emph{function family} or \emph{ansatz}, characterized by \emph{free parameters}.

  \begin{gather*}
    \left\{ f^\theta|\, f^\theta: S, \Theta \rightarrow labels \right\}\\
    \Theta \subseteq \mathbb{R}^p
  \end{gather*}

  \begin{block}{Training a supervised learning model - practical definition}
    Find $\theta$ s.t. $f_\theta$ best approximizes $f$ w.r.t. a metric of error or \emph{loss}.
  \end{block}

  \begin{table}[h]
    \centering
    \begin{tabular}{ccc}
      Mean squared error & $ \sum_{i=1}^N \left( y_i - f_\theta\left( x_i \right) \right)^2$ & Regression\\
      Negative log-likelihood & $ -\log\left( \mathrm{smtg} \right)$& Classification
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Supervised learning algorithms}
  \begin{itemize}
    \item Neural networks (this lecture)
    \item Support vector machines (next lecture)
    \item Naive Bayes
    \item Linear regression
    \item Logistic regression
    \item $k$-nearest neighbours
    \item Random forest
    \item ...
  \end{itemize}
  
  \note{Read some algorithms descriptions \href{here}{https://www.ibm.com/topics/supervised-learning}
  }
\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=.7\textwidth]{tikz/cnn.pdf}
  \end{figure}

  \begin{itemize}
    \item $f_\theta = f\left( W^{\left( 1 \right)}, b^{\left( 1 \right)}W^{\left( 2 \right)}, b^{\left( 2 \right)}, \dots, W^{\left( l \right)}, b^{\left( l \right)} \right)$
    \item Layered structure: $x^{\left( l+1 \right)} = \sigma\left( W^{\left( l \right)}x^{\left( l \right)} + b^{\left( l \right)} \right)$
  \end{itemize}

  Furthermore, NNs are universal function approximators\cite{nielsenneural}.

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Training - backpropagation}
  
  backpropagation
  \begin{itemize}
    \item Training neural networks uses gradient-based algorithms
    $$
      \theta^{(t+1)} = \theta^{(t)} - \eta\nabla\theta^{(t)}
    $$
    \item Computation graph: 3 steps
    \begin{itemize}
      \item forward direction - get the loss value
      \item evaluate local gradients
      \item backward direction: multiplications (chain rule)
    \end{itemize}
    \item This is different than QNNs, which breaks the computational graph at the model level
    \item \begin{itemize}
      \item More later \dots
    \end{itemize}
    \note{for a qnn, we need to evaluate so many quantum circuits: $n_{parameters} * 2 * n_{shots}$, where 2 means the evaluation for $\pm \frac{\pi}{2}$}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Classical neural networks}
  \framesubtitle{Backpropagation example}


  \note{leave it blank for drawing the computational graph like in handwritten notes}

\end{frame}


\begin{frame}
  \frametitle{Quantum machine learning}
  \framesubtitle{Some informal definitions}
  
  \footnotesize
  \begin{block}{\textbf{Quantum-enhanced machine learning}}
    Quantum computation to speed-up classical machine learning operations.

    \ \\
    \emph{Example}: SVM with quantum linear systems solver algorithm

    \ \\
    \emph{Limitations}
    \begin{itemize}
      \item Input/output problem (QRAM?)
      \item De-quantization argument: for a fair comparison, \emph{both classical and quantum} algorithm must have access to a QRAM $\rightarrow$ classical algorithm also runs in $O\left( \log(N) \right)$
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Quantum machine learning}
  \framesubtitle{Some informal definitions}

  \begin{block}{\textbf{Machine learning in quantum feature spaces}}
    Information is \emph{encoded} into quantum states and classical algorithms find the optimal model.

    \begin{itemize}
      \item Heuristic
    \end{itemize}
  \end{block}

\emph{Note}: From here on, we will be focusing on ML in quantum feature spaces. For a review of quantum-enhanced ML, see \cite{Biamonte_2017}

\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{The advantage POV}

  As with all quantum algorithms, we ask ourselves: \emph{how is QML better than ML?}

  \ \\
  Quantum advantage can be proved by
  \begin{itemize}
    \item Mapping data to certain states
    \item These states are hard to produce classically, but easy to produce (BQP) via a quantum algorithm
    \begin{itemize}
      \item e.g. by solving the discrete log problem \cite{Liu_2021}
    \end{itemize}
    \item We can compute inner products efficiently\footnote{the key idea of kernel theory, more in the next lecture}
  \end{itemize}



\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{The advantage POV}
  However\dots

  \begin{itemize}
    \item Are there any \emph{practically interesting} dataset in which QML can achieve advantage?
    \pause
    \item Is advantage even the right thing to look for?
  \end{itemize}


\end{frame}


\begin{frame}
  \frametitle{Why QML?}
  \framesubtitle{Is quantum advantage the right goal for quantum machine learning? \cite{Schuld_2022}}
  
  Advantage is just one of the possible points of view and possibly not the most useful one.
  \begin{enumerate}
    \item What is(are) the fundamental building block(s) of QML models?
    \begin{itemize}
      \item a.k.a. what is the \emph{quantum perceptron}?
    \end{itemize}
    \item Can classical learning theory give us guarantees on the learning performance of QML models?
    \begin{itemize}
      \item Yes, and the link is kernel theory.
    \end{itemize}
    \item Can we automate the learning process of (certain) QML models?
    \begin{itemize}
      \item In particular, automatic differentiation.
    \end{itemize}
  \end{enumerate}

\end{frame}


\begin{frame}
  
  \centering
  \Large
  Quantum neural networks

\end{frame}


\begin{frame}
  \frametitle{The quantum neural network model}

  3 ingredients
  \begin{itemize}
    \item A data encoding unitary: $U(\mathbf{x})$
    \item A variational unitary (anzatz): $W\left( \mathbf{\theta} \right)$
    \item An observable (measurement operator): $\mathcal{M}$
  \end{itemize}

  \[f(\mathbf{x}, \theta) = \ev{\mathcal{M}}{\psi\left( \mathbf{x}, \theta \right)},\]
  where
  \[\ket{\psi\left( \mathbf{x}, \theta \right)} = W\left( \theta \right) U(\mathbf{x}) \ket{0}\]
  and
  \[\mathbf{x}\in \R^m,\: \theta\in \R^p\]

\end{frame}


\begin{frame}
  \frametitle{The quantum neural network model}

  % \begin{figure}[h]
  %   \mbox{
  %   \Qcircuit @C=1em @R=.7em {
  %       \lstick{\ket{0}} & & \multigate{3}{U(\mathbf{x})} & \multigate{3}{W(\theta)} & & \multimeasure{3}{\mathcal{M}}\\
  %       \lstick{\ket{0}} & & \ghost{U(\mathbf{x})} & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}\\
  %       & \vdots & \ghost{U(\mathbf{x})} & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}\\
  %       \lstick{\ket{0}} & & \ghost{U(\mathbf{x}) & \ghost{W(\mathbf{\theta})} & \ghost{\mathcal{M}}}
  %       }
      
  % \end{figure}

  \note{left blank for drawing the generic circuit}
        
\end{frame}


\begin{frame}
  \frametitle{A simple QNN example}

  
  \note{draw the simple RX(x)-RZ($\theta$) circuit with X measurement}

\end{frame}


\begin{frame}
  \frametitle{QNN model evaluation}

  The model $f = \ev{\mathcal{M}}_{\mathbf{x},\theta}$ is the expectation value of the observable $\mathcal{M}$ of the state $\psi\left( \mathbf{x}, \mathbf{\theta} \right) $
  \begin{itemize}
    \item In practice, we can compute $\ev{\mathcal{M}_{\mathbf{x},\theta}}$ if we know how to measure in $\mathcal{M}$ basis
    \item See previous slide where we measured in the $X$ basis.
    \item Alternatively, we use Hadamard\footnote{\url{https://en.wikipedia.org/wiki/Hadamard_test_(quantum_computation)}} or SWAP\footnote{\url{https://en.wikipedia.org/wiki/Swap_test}} tests
    \item Finite sampling: $O(1/\varepsilon^2)$ measurement to estimate $\ev{M}$ up to precision $\varepsilon$
    \begin{itemize}
      \item Due to Chebyshev's inequality
      \item In practice, a convergence study is necessary
    \end{itemize}
  \end{itemize}
  

\end{frame}


\begin{frame}
  \frametitle{QNN model training}

  Training is hybrid, as for all variational algorithms. At each epoch
  \begin{itemize}
    \item (Quantum step) \emph{Evaluate} the model $f = \ev{\mathcal{M}}_{\mathbf{x},\theta^{(t)}}$ and \emph{maybe} $\grad_\theta f$.
    \begin{itemize}
      \item More on gradients later on
    \end{itemize}
    \item (Classical step 1) \emph{Forward step} in the computational graph
    \begin{itemize}
      \item i.e. we evaluate a loss function $L\left( \theta^{(t)} \right)$ and the local gradients
    \end{itemize}
    \item (Classical step 2) \emph{Backward step} in the computational graph
    \item (Classical step 3) \emph{Parameter update}
    \[\mathrm{GD:}\qquad \theta^{(t+1)} = \theta^{(t)} - \eta\nabla\theta^{(t)}\]
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{QNN model training}

  \centering
  \color{red} Do you think it is a good idea to evaluate $\grad_\theta f$ with finite differences? Why? Why not?

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}

  A good choice of data embedding can make the difference in terms of
  \begin{itemize}
    \item Runtime of the QML routine
    \begin{itemize}
      \item how does the number of operations scale in the number of features $N$ and dataset dimension $M$
    \end{itemize}
    \item Learning performance
    \begin{itemize}
      \item Encoding data means to map them to a high-dimensional Hilbert space
      \item E.g., in classification, we can linearly separate the data of different classes\footnote{even better, with high \emph{margin}}, but also completely shuffle them!
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Data encoding as a feature map}

  \[\phi\left( \mathbf{x} \right) : \mathcal{X}\rightarrow \mathcal{F}\subseteq \mathcal{H}^{2^n}\]

  \ \\
  \centering
  \includegraphics[width=.6\textwidth]{pics/feature-maps.png}
\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Amplitude encoding}

  We have $\mathbf{x}\in \R^N$. To make it a valid quantum state:
  \begin{enumerate}
    \item Normalize: $\frac{\mathbf{x}}{\norm{x}_2}$
    \item Pad until the closest power of 2: $x_N,\dots,x_{2^n-1}=0$
  \end{enumerate}

  Amplitude encoding (AE) is the identity transformation of this preprocessed state
  \[\phi(\mathbf{x})=\ket{x}=\sum_{i=0}^{N-1}x_i\ket{i}\]

  \begin{itemize}
    \item AE is a linear transformation: if data is not linearly separable in the (normalized and padded) original space, then it won't be in the mapped space.
    \item num. qubits: $\log(N)$
    \item runtime: $O(N)$\cite{mottonen2004transformation} 
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Amplitude encoding}

  By adding an additional $\log(M)$ qubits, we can also encode data in superposition

  \[\ket{\psi_\mathbf{X}} = \frac{1}{\sqrt{M}}\sum_{m=0}^{M-1}\sum_{i=0}^{N-1} x_i\ket{i}\ket{m}\]

  \begin{itemize}
    \item num. qubits: $\log(NM)$
    \item runtime: $O(NM)$
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  For basis encoding, we first need to choose a binary representation of the features $x_i$. E.g., in floating point
  \[b_i=b_{i,s} b_{i,n_e-1}\dots b_{i,0}b_{i,n_m-1}\dots b_{i,0}\]

  \ \\
  With $\tau = 1+n_e+n_m$ we can encode \emph{each feature of each datapoint} into a different basis state. If we concatenate the features:
  \[\phi(\mathbf{x})=\ket{b^0\dots b^{N-1}}\]

  \ \\
  Of course, we can also create superpositions
  \[\ket{\psi_\mathbf{X}} = \frac{1}{\sqrt{M}}\sum_{i=1}^M \ket{\mathbf{x}^{(i)}} \]

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  \begin{minipage}{\textwidth}
    \begin{table}[t]
      \footnotesize
      \centering\begin{tabular}{ccc}
        & Single sample & Dataset\\
        \hline\\
        num. qubits & $N\tau$ & $N\tau$\\
        runtime & $O(N\tau)$ & $O\left( Mn\tau \right)$\cite{ventura1998quantum}\\
        \hline
        
      \end{tabular}
    
    \end{table}
    
  \end{minipage}

  \vspace{3cm}
  \begin{minipage}{\textwidth}
    \note{here include a simple example for basis encoding. For instance $\mathbf{x} = (101, 001)$}
    
  \end{minipage}
  

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Basis encoding}

  \centering
  \color{red} If basis encoding achieves perfect orthogonality between datapoints, why shouldn't we always use it for classification?

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Time-evolution encoding}

  

\end{frame}


\begin{frame}
  \frametitle{Encoding classical data in quantum states}
  \framesubtitle{Hamiltonian encoding of a dataset}

  

\end{frame}

\section{Conclusion}
\begin{frame}[fragile]{animation}
  \vfill
  Some commands take optional arguments in the form of \verb|<x-y>|,
  where \verb|x| is the first `sub-frame' on which the context is shown,
  and \verb|y| is the last. \verb|x| or \verb|y| can be replaced by \verb|+|,
  referring to `the next sub-frame'. 
  \vfill
  \begin{columns}[onlytextwidth]
  \begin{column}{.5\textwidth}
    \begin{enumerate}
      \item<+-> uncovered\ldots
      \item<+-> one\ldots
      \item<+-> by\ldots
      \item<+-> one.
    \end{enumerate}
    \end{column}
  \begin{column}{.5\textwidth}
      Using only:\only<1>{1}\only<2>{2}\only<3>{3}

      Using onslide:\onslide<1>{1}\onslide<2>{2}\onslide<3>{3}

      Using pause:\pause1\pause2\pause3
  \end{column}
  \end{columns}
  \vfill
  For more advanced animations, see \S 14 of the manual:\\
  \url{https://www.ctan.org/pkg/beamer}
  % \url{https://www.ctan.org/pkg/animate}\\
  % \url{https://www.ctan.org/pkg/media9}
  \vfill
  % \transduration{2} automatic progression of slides
  \transpush<1>
\end{frame}

\begin{frame}
  Thanks for your attention.

  A digital version of this presentation can be found here:
  \vfill
  \url{https://gitlab.com/novanext/tudelft-beamer} 
  \vfill  
  \centering
  \qrcode{https://gitlab.com/novanext/tudelft-beamer}
  \vfill
\end{frame}


\begin{frame}[allowframebreaks,t]{\bibname}
	% the 'I' is caused by 'allowframebreaks'
	\AtNextBibliography{\footnotesize}% or in the preamble \AtBeginBibliography{\small}
	\printbibliography
\end{frame}


\end{document}

